{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRON - QUESTION 1\n",
    "\n",
    "\\begin{array}{l}g \\in \\partial f_{k}(x) \\\\ \\Rightarrow f_{k}(z) \\geq f_{k}(x)+g^{\\top}(z-x)\\end{array}\n",
    "\n",
    "\\begin{aligned} f(z) &=\\max _{i=1, \\ldots, m}\\left\\{f_{i}(z)\\right\\} \\\\ & \\geq f_{k}(z) \\\\ & \\geqslant f_{k}(x)+g^{\\top}(z-x) \\\\ &=f(x)+g^{\\top}(z-x) \\\\ \\Rightarrow g & \\in \\partial f(x) \\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRON - QUESTION 2\n",
    "\n",
    "$$g=\\left\\{\\begin{array}{cl}-y x & \\text { if } y w^{\\top} x<1 \\\\ 0 & \\text { otherwise }\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRON - QUESTION 3\n",
    "\n",
    "\\begin{array}{l}\\qquad\\left\\{x | w^{\\top} x\\right\\} \\text { is a separating hyperplane for } D \\\\ \\Rightarrow y_{i} w^{\\top} x_{i}>0 \\quad \\forall \\leqslant i \\leqslant n \\\\ \\Rightarrow \\operatorname{sgn}\\left(w^{\\top} x_{i}\\right)=\\operatorname{sgn}\\left(y_{i}\\right) \\\\ \\Rightarrow-y_{i} \\operatorname{sgn}\\left(w^{\\top} x_{i}\\right)=-y_{i} \\operatorname{sgn}\\left(y_{i}\\right) \\leq 0 \\\\ \\text { Average perceptron loss } = \\frac{1}{n} \\sum_{i=1}^{n} l(\\hat{y}, y) \\\\ =\\frac{1}{n} \\sum_{i=1}^{n} \\max \\{0,-\\hat{y} y\\} \\\\ =\\frac{1}{n} \\sum_{i=1}^{n} \\max \\left\\{0,-y_{i} \\operatorname{sgn}\\left(w^{\\top} x_{i}\\right)\\right\\} \\\\ =0\\end{array}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRON - QUESTION 4\n",
    "\n",
    "The weight update step of the SSGD algorithm is given by:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}w_{(t+1)}=w_{(t)}-\\eta g \\\\ \\text { Choose } \\eta=1 \\text { and } g=\\left\\{\\begin{array}{cl}-y_{i} x_{i} & \\text { if } y_{i} w_{(t)}^{\\top} x_{i}<1 \\\\ 0 & \\text { othenwise }\\end{array}\\right.\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\\text { Then the SSGD update step can be written as the following conditional statement } \\\\ \\text { if }\\left(y_{i} w_{(t)}^{\\top} x_{i}<1\\right) \\\\ \\qquad \\begin{array}{l}w_{(t+1)}=w_{(t)}+y_{i} x_{i} \\\\ \\text { else } \\\\ w_{(t+1)}=w_{(t)}\\end{array} \\\\ \\text { This is exactly the same condition used in the Perceptron Algorithm }\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERCEPTRON - QUESTION 5\n",
    "\n",
    "This follows by induction on k in the Perceptron Algorithm. <br>\n",
    "Let P(t) be the proposition that the output of the perceptron algorithm is a linear combination of input points after t iterations. <br>\n",
    "P(1) is true since \n",
    "\\begin{aligned} w^{(1)} &=w^{(0)}+y_{1} x_{1} \\\\ &=y_{1} x_{1} \\end{aligned}\n",
    "or remains unchanged. <br>\n",
    "$w$ is initialized by a zero vector. <br>\n",
    "\n",
    "Assume P(k) is true i.e. $w^{(k)}$ is L.C. of $x_i$ <br>\n",
    "In the case where $y_{i} x_{i}^{\\top} w^{(k)}>0$, w stays unchanged. <br>\n",
    "In the case where $y_{i} x_{i}^{\\top} w^{(k)} \\leq 0$, $$w^{(k+1)}=w^{(k)}+y_{i} x_{i}$$ <br>\n",
    "Since $w^{(k)}$ is a L.C., $w^{(k+1)}$ is also a L.C. of $x_i$. <br>\n",
    "Therefore, P(k+1) is true. <br>\n",
    "\n",
    "The result follows by principle of mathematical induction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPARSE REPRESENTATION - QUESTION 1 and 2\n",
    "(implementation for QUESTION 2 has been baked into QUESTION 1 to make it more optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from copy import copy\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_DIR = '../data/svm/data/'\n",
    "POS_DATA_DIR = DATA_DIR + 'pos/'\n",
    "NEG_DATA_DIR = DATA_DIR + 'neg/'\n",
    "TRAIN_SPLIT_COUNT = 1500\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read review file into a list of strings\n",
    "# each string = a token from the review\n",
    "def read_file(filepath):\n",
    "    f = open(filepath)\n",
    "    lines = f.read().split(' ')\n",
    "    symbols = '${}()[].,:;+-*/&|<>=~\" '\n",
    "    words = map(lambda Element: Element.translate(str.maketrans(\"\", \"\", symbols)).strip(), lines)\n",
    "    words = filter(None, words)\n",
    "    return Counter(list(words))\n",
    "\n",
    "# return list of file names in a directory\n",
    "# skips directories\n",
    "def list_dir_files(dirpath):\n",
    "    file_names = []\n",
    "    children = os.listdir(dirpath)\n",
    "\n",
    "    for child in children:\n",
    "        if os.path.isfile(os.path.join(dirpath, child)):\n",
    "            file_names.append(child)\n",
    "    \n",
    "    return file_names\n",
    "\n",
    "# read all files in the directory\n",
    "def read_dir(dirpath):\n",
    "    file_contents = []\n",
    "    file_names = list_dir_files(dirpath)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        filepath = os.path.join(dirpath, file_name)\n",
    "        file_content = read_file(filepath)\n",
    "        file_contents.append(file_content)\n",
    "\n",
    "    return file_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews = read_dir(POS_DATA_DIR)\n",
    "neg_reviews = read_dir(NEG_DATA_DIR)\n",
    "\n",
    "NUM_POS_REVIEWS = len(pos_reviews)\n",
    "NUM_NEG_REVIEWS = len(neg_reviews)\n",
    "\n",
    "pos_review_points = list(zip(pos_reviews, [1] * NUM_POS_REVIEWS, range(1, NUM_POS_REVIEWS + 1)))\n",
    "neg_review_points = list(zip(neg_reviews, [-1] * NUM_NEG_REVIEWS, range(1, NUM_NEG_REVIEWS + 1)))\n",
    "\n",
    "all_points = pos_review_points.copy()\n",
    "all_points.extend(neg_review_points)\n",
    "random.shuffle(all_points)\n",
    "\n",
    "train = all_points[:TRAIN_SPLIT_COUNT]\n",
    "test = all_points[TRAIN_SPLIT_COUNT:]\n",
    "\n",
    "train_unzip = list(zip(*train))\n",
    "train_X = list(train_unzip[0])\n",
    "train_Y = list(train_unzip[1])\n",
    "train_indices = list(train_unzip[2])\n",
    "\n",
    "test_unzip = list(zip(*test))\n",
    "test_X = list(test_unzip[0])\n",
    "test_Y = list(test_unzip[1])\n",
    "test_indices = list(test_unzip[2])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 1\n",
    "\n",
    "$$\n",
    "\\nabla_{\\omega} J_{i}(\\omega)=\\left\\{\\begin{array}{lll}\\lambda w-y_{i} x_{i} & \\text { if } y_{i} w^{\\top} x_{i}<1 \\\\ \\lambda \\omega & \\text { if } y_{i} w^{\\top} x_{i}>1 \\\\ \\text { undefined } & \\text { if } y_{i} w^{\\top} x_{i}=1\\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 2\n",
    "\n",
    "$$\n",
    "\\partial J_{i}(\\omega)=\\partial\\left(\\frac{\\lambda}{2}\\left\\|_{w}\\right\\|^{2}\\right)+\\partial\\left(\\max \\left\\{0,1-y_{i} w^{\\top} x_{i}\\right\\}\\right) \\\\\n",
    "g=\\left\\{\\begin{array}{ll}\\lambda w-y_{i} x & \\text { if } y_{i} w^{\\top} x_{i}<1 \\\\ \\lambda{w} & \\text { otherwise }\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Using results from PERCEPTRON - QUESTION 2, subdifferential of sum of convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 3\n",
    "\n",
    "The subgradient update expression is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}w^{(t+1)}=w^{(t)}-\\eta g \\\\ g=\\left\\{\\begin{array}{ll}\\lambda w^{(t)}-y_{i} x_{i} & \\text { if } y_{i} w^{\\top} x_{i}<1 \\\\ \\lambda w^{(t)} & \\text { otherwise }\\end{array}\\right.\\end{array} \\\\\n",
    "\n",
    "w^{(t+1)}=\\left\\{\\begin{array}{ll}w^{(t)}-\\eta\\left(\\lambda w^{(t)}-y ; x_{i}\\right) & \\text { if } y_{i} w^{\\top} x_{i}<1 \\\\ w^{(t)}-\\eta \\lambda w^{(t)} & \\text { otherwise }\\end{array}\\right. \\\\\n",
    "\n",
    "w^{(t+1)}=\\left\\{\\begin{array}{ll}(1-\\eta \\lambda) w^{(t)}+\\eta y_{i} x_{i} & \\text { if } y_{i} w^{T} x_{i}<1 \\\\ (1-\\eta \\lambda) w^{(t)} & \\text { otherwise }\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "This corresponds exactly to the conditional statement in the pseudocode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dot product between sparse representation of dictionaries/counters\n",
    "\"\"\"\n",
    "def sparse_dot(d1, d2):\n",
    "    if len(d1) < len(d2):\n",
    "        return sparse_dot(d2, d1)\n",
    "    \n",
    "    return sum(d1[k] * v for k, v in d2.items())\n",
    "\n",
    "\"\"\"\n",
    "Inplace linear combination update for d1 += z * d2\n",
    "\"\"\"\n",
    "def sparse_lc(d1, z, d2):\n",
    "    for k, v in d2.items():\n",
    "        d1[k] += z * v\n",
    "\n",
    "\"\"\"\n",
    "Sign function impl\n",
    "\"\"\"\n",
    "def sign(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    elif x < 0:\n",
    "        return -1\n",
    "    return 0\n",
    "\n",
    "def hinge_loss(X, y, w):\n",
    "    m = len(X)\n",
    "    loss = 0\n",
    "\n",
    "    for j in range(m):\n",
    "        loss += max(0, 1 - y[j] * sparse_dot(w, X[j]))\n",
    "    \n",
    "    return loss / m\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pegasos():\n",
    "    def __init__(self, l2_reg = 1):\n",
    "        self.w = Counter()\n",
    "        self.l2_reg = l2_reg\n",
    "    \n",
    "    def fit(self, X, y, max_epochs = 1000):\n",
    "        epoch = 0\n",
    "        m = len(X)\n",
    "        t = 1\n",
    "        while epoch < max_epochs:\n",
    "            for j in range(m):\n",
    "                x_j = X[j]\n",
    "                y_j = y[j]\n",
    "\n",
    "                t += 1\n",
    "                step_size = 1 / (t * self.l2_reg)\n",
    "                \n",
    "                if y_j * sparse_dot(self.w, x_j) < 1:\n",
    "                    sparse_lc(self.w, -step_size * self.l2_reg, self.w)\n",
    "                    sparse_lc(self.w, step_size * y_j, x_j)\n",
    "                else:\n",
    "                    sparse_lc(self.w, -step_size * self.l2_reg, self.w)\n",
    "            \n",
    "                \n",
    "            epoch += 1\n",
    "        \n",
    "        return self.w, epoch\n",
    "    \n",
    "    def predict(self, X, y = None):\n",
    "        return sparse_dot(X, self.w)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        num_misclass = 0\n",
    "        m = len(X)\n",
    "        for j in range(m):\n",
    "            if sign(sparse_dot(X[j], self.w)) != y[j]:\n",
    "                num_misclass += 1\n",
    "        \n",
    "        return num_misclass / m\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 5 and 7\n",
    "(implementation of QUESTION 7 is baked into the class implementation for Pegasos) <br>\n",
    "\n",
    "\\begin{array}{l}\\qquad W_{(t+1)}=W_{(t)}+\\frac{1}{s_{(t+1)}} \\eta_{t} y_{j} x_{j} \\\\ \\Leftrightarrow \\quad s_{(t+1)} W_{(t+1)}=s_{(t+1)} W_{(t)}+\\eta_{t} y_{j} x_{j} \\\\ \\Leftrightarrow \\quad w_{(t+1)}=\\left(1-\\eta_{t} \\lambda\\right) s_{(t)} W_{(t)}+\\eta_{t} y_{j} x_{j} \\\\ \\Leftrightarrow w_{(t+1)}=\\left(1-\\eta_{t} \\lambda\\right) w_{(t)}+\\eta_{t} y_{j}{x_j}\\end{array} <br>\n",
    "\n",
    "Therefore, both approaches are equivalent as long as $s_{(t+1)} \\neq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pegasos_opt():\n",
    "    def __init__(self, l2_reg = 1):\n",
    "        self.w = Counter()\n",
    "        self.s = 1\n",
    "        self.l2_reg = l2_reg\n",
    "    \n",
    "    def fit(self, X, y, max_epochs = 1000, min_obj_decrease=1e-8):\n",
    "        epoch = 0\n",
    "        m = len(X)\n",
    "        t = 1\n",
    "        s_ = 1\n",
    "        obj_decrease = min_obj_decrease + 1.\n",
    "        obj_val = hinge_loss(X, y, self.w)\n",
    "\n",
    "        while obj_decrease > min_obj_decrease and epoch < max_epochs:\n",
    "            obj_old = obj_val\n",
    "            for j in range(m):\n",
    "                x_j = X[j]\n",
    "                y_j = y[j]\n",
    "\n",
    "                t += 1\n",
    "                step_size = 1 / (t * self.l2_reg)\n",
    "\n",
    "                s_ = (1 - step_size * self.l2_reg) * s_\n",
    "\n",
    "                if s_ == 0:\n",
    "                    s_ = 1\n",
    "                    self.w = Counter()\n",
    "                else:\n",
    "                    if y_j * sparse_dot(self.w, x_j) < 1:\n",
    "                        sparse_lc(self.w, step_size * y_j/s_, x_j)\n",
    "            \n",
    "            obj_val = hinge_loss(X, y, self.w)\n",
    "            obj_decrease = abs(obj_old - obj_val)\n",
    "            epoch += 1\n",
    "        \n",
    "        sparse_lc(self.w, s_ - 1, self.w)\n",
    "\n",
    "        return self.w, epoch\n",
    "    \n",
    "    def predict(self, X, y = None):\n",
    "        return sparse_dot(X, self.w)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        num_misclass = 0\n",
    "        m = len(X)\n",
    "\n",
    "        for j in range(m):\n",
    "            if sign(sparse_dot(X[j], self.w)) != y[j]:\n",
    "                num_misclass += 1\n",
    "        \n",
    "        return num_misclass / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Running <class '__main__.Pegasos'>\nTime taken = 26.074629068374634\n\n('will', 0.06564478507164302)\n('and', 0.06564478507164277)\n('most', 0.06231256247917362)\n('life', 0.06231256247917344)\n('as', 0.05764745084971683)\n('very', 0.055981339553482154)\n('well', 0.05431522825724774)\n('is', 0.05398200599800054)\n('world', 0.04865044985005)\n('war', 0.044318560479840056)\n('also', 0.04431856047984003)\n('many', 0.043318893702099265)\n('quite', 0.04298567144285229)\n('their', 0.04165278240586479)\n('best', 0.0393202265911364)\n('both', 0.03832055981339565)\n('love', 0.03765411529490172)\n('we', 0.03498833722092639)\n('way', 0.032989003665444945)\n('with', 0.03265578140619793)\n\n\nRunning <class '__main__.Pegasos_opt'>\nTime taken = 0.3313291072845459\n\n('most', 0.043652115961350546)\n('well', 0.037987337554142186)\n('very', 0.03498833722092343)\n('great', 0.0339886704431791)\n('also', 0.03365544818393573)\n('world', 0.03198933688770467)\n('life', 0.031322892369203714)\n('quite', 0.030656447850716972)\n('you', 0.029323558813729278)\n('many', 0.028323892035984954)\n('war', 0.02732422525824063)\n('best', 0.02699100299899726)\n('american', 0.025991336221252936)\n('love', 0.025658113962009566)\n('seen', 0.025324891702766195)\n('will', 0.02365878040653513)\n('story', 0.022992335888034177)\n('we', 0.022325891369547435)\n('your', 0.021992669110289853)\n('their', 0.021992669110289853)\n\n\n"
    }
   ],
   "source": [
    "\n",
    "for impl in [Pegasos(1), Pegasos_opt(1)]:\n",
    "    print('Running ' + str(type(impl)))\n",
    "    start = time.time()\n",
    "    w, e = impl.fit(train_X, train_Y, max_epochs = 2)\n",
    "    end = time.time()\n",
    "    \n",
    "    print('Time taken = ' + str(end - start))\n",
    "    print()\n",
    "    for x in w.most_common(20):\n",
    "        print(x)\n",
    "    print()\n",
    "    print()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM with PEGASOS - QUESTION 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_dict_values(dict_, k, order = 0):\n",
    "    \"\"\"\n",
    "    Sorts a dict and returns the top k elems as a list of tuples (ordered by values)\n",
    "    order = 0 for ascending, order = 1 for descending\n",
    "    \"\"\"\n",
    "    \n",
    "    # map the order from {0, 1} to {-1, 1}\n",
    "    order = order * 2 - 1\n",
    "    \n",
    "    d = {k:v for k, v in sorted(dict_.items(), key = lambda item : -order*item[1])}\n",
    "    ret_list = []\n",
    "    for x in list(d)[0:(k + 1)]:\n",
    "        ret_list.append((x, d[x]))\n",
    "    \n",
    "    return ret_list\n",
    "\n",
    "def print_top_dict_values(dict_, k, order = 0):\n",
    "    \"\"\"\n",
    "    Prints the top k elems of the dictionary (ordered by values)\n",
    "    order = 0 for ascending, order = 1 for descending\n",
    "    \"\"\"\n",
    "    \n",
    "    for x in get_top_dict_values(dict_, k, order):\n",
    "        print(\"{}\\t{}\".format(x[1], x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda_list = [10**-3, 10**-2, 0.1, 0.6, 0.8, 1, 1.05, 1.08, 1.2, 1.5, 1.8, 2, 2.2]\n",
    "lambda_list = [1, 1.005, 1.01, 1.03, 1.05]\n",
    "config_val_loss_map = {}\n",
    "\n",
    "for lamba_reg in lambda_list:\n",
    "    impl = Pegasos_opt(lamba_reg)\n",
    "    w, e = impl.fit(train_X, train_Y, max_epochs = 500)\n",
    "    loss = impl.score(test_X, test_Y)\n",
    "    config_val_loss_map[(lamba_reg, e)] = loss\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.16\t(1, 39)\n0.16\t(1.005, 39)\n0.16\t(1.01, 39)\n0.16\t(1.03, 39)\n0.16\t(1.05, 39)\n"
    }
   ],
   "source": [
    "print_top_dict_values(config_val_loss_map, len(lambda_list), order = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best validation loss is observed for $\\lambda \\in [1, 1.05]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNELS - QUESTION 1\n",
    "\n",
    "Consider $\\phi$ as the binary vectorizer. In terms of the sparse representation, $\\phi(x)$ is a dictionary where a word appears in the dictionary if and only if it appears in the document and the value against is key is always 1. <br>\n",
    "This fits well into the sparse inner product we have used the Pegasos implementation. <br>\n",
    "Keeping the same definition of the inner product, we see that this featurization counts the number of common words in two sparse vectors. <br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\\phi\\left(x_{1}\\right)= & \\sum \\alpha_{i} w_{1 i} & , \\alpha_{i} \\in\\{0,1\\} \\\\ \\phi\\left(x_{2}\\right)= & \\sum \\beta_{i} w_{2 i} & ; \\beta_{i} \\in\\{0,1\\}\\end{array} \\\\\n",
    "\n",
    "\\phi\\left(x_{1}\\right)^{\\top} \\phi\\left(x_{2}\\right)=\\sum \\alpha_{i} \\beta_{i}=\\sum_{\\alpha_{i} \\neq 0 \\atop \\beta_{i} \\neq 0} 1\n",
    "$$\n",
    "\n",
    "The inner product thus counts the number of common words in both documents.\n",
    "\n",
    "$$\n",
    "\\phi\\left(x_{1}\\right)^{\\top} \\phi\\left(x_{2}\\right) = k(x_1, x_2)\n",
    "$$\n",
    "\n",
    "Therefore, k is a kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNELS - QUESTION 2\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}k(x, z)=x^{\\top} z \\\\ f(x)=1 /\\|x\\|_{2} \\\\ \\Rightarrow k(x, x)=x^{\\top} x \\text { is a kernel } \\\\ \\Rightarrow f(x) f(x)=k(x, x)=\\frac{x^{\\top} x}{\\|x\\|_{2}^{2}}=1 \\text { is a kernel }(1)\\end{array} \\\\\n",
    "\n",
    "\\begin{aligned} & k(x, 2) \\text { is a kernel } \\\\ \\Rightarrow & f(x) f(z) k(x, z)=\\frac{x^{\\top} z}{\\|x\\|_{2}\\|z\\|_{2}} \\quad \\text { is a kernel }(2) \\end{aligned} \\\\\n",
    "\n",
    "\\begin{array}{l}\\quad \\text{By} \\quad 1,2 \\\\ f(x) f(x) k(x, x)+f(x) f(z) k(x, z)=1+\\frac{x^{\\top} z}{\\|x\\|_{2}\\|z\\|_{2}} \\text { is a kernel } \\\\ \\Rightarrow\\left(1+\\frac{x^{\\top} z}{\\|x\\|_{2}\\|z\\|_{2}}\\right)\\left(1+\\frac{x^{\\top} z}{\\|x\\|_{2}\\|z\\|_{2}}\\right)\\left(1+\\frac{x^{\\top} z}{\\|x\\|_{2}\\|z\\|_{2}}\\right) \\text { is a kernel} \\\\ \\Rightarrow \\quad\\left(1+\\frac{x^{\\top} z}{\\|x\\|_{2}\\|z\\|_{2}}\\right)^{3} \\text { is a kernel }\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNEL PEGASOS - QUESTION 1\n",
    "\n",
    "$$\n",
    "\\begin{aligned} y_{j}\\left\\langle w^{(t)}, x_{j}\\right\\rangle &=y_{j}\\left\\langle x_{j}, \\sum_{i=1}^{n} \\alpha_{i}^{(t)} x_{i}\\right\\rangle \\\\ &=y_{j} \\sum_{i=1}^{n} \\alpha_{i}^{(t)}\\left\\langle x_{j}, x_{i}\\right\\rangle \\\\ &=y_{j} K_{j} \\alpha^{(t)} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNEL PEGASOS - QUESTION 2\n",
    "\n",
    "$$\n",
    "\\begin{aligned} w^{(t+1)}=&\\left(1-\\frac{1}{t}\\right) w^{(t)} \\\\ \\sum_{i=1}^{n} \\alpha_{i}^{(t+1)} x_{i} &=\\left(1-\\frac{1}{t}\\right) \\sum_{i=1}^{n} \\alpha_{i}^{(t)} x_{i} \\\\ &=\\sum_{i=1}^{n}\\left(1-\\frac{1}{t}\\right) \\alpha_{i}^{(t)} x_{i} \\\\ \\alpha^{(t+1)}=&\\left(1-\\frac{1}{t}\\right) \\alpha^{(t)} \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KERNEL PEGASOS - QUESTION 3\n",
    "\n",
    "$$\n",
    "\\begin{aligned} w^{(t+1)} &=\\left(1-\\frac{1}{t}\\right) w^{(t)}+\\eta y_{j} x_{j} \\\\ \\sum_{i=1}^{n} \\alpha_{i}^{(t+1)} x_{i} &=\\left(1-\\frac{1}{t}\\right) \\sum_{i=1}^{n} \\alpha_{i}^{(t)} x_{i}+\\sum_{i=1}^{n} \\eta y_{j} \\delta_{i j} x_{i} \\quad\\left(\\delta_{i j}=\\text { kronecker delta }\\right) \\\\ &=\\sum_{i=1}^{n}\\left[\\left(1-\\frac{1}{t}\\right) \\alpha_{i}^{(t)}+\\eta y_{j} \\delta_{i j}\\right] x_{i} \\\\ \\alpha^{(t+1)} &=\\left(1-\\frac{1}{t}\\right) \\alpha^{(t)}+\\eta y_{j} \\hat{e}_{j} \\quad\\left(\\hat{e}_{j}=\\text { canonical basis vector in direction j }\\right) \\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "input lambda > 0.\n",
    "Choose alpha_1 = 0, t = 0\n",
    "while termination condition is not met:\n",
    "    for j = 1, 2, ..., m:\n",
    "        t = t + 1\n",
    "        eta = 1 / (lambda * t)\n",
    "\n",
    "        if y_j * dot(K_j, alpha_{t}) < 1:\n",
    "            alpha_{t+1} = (1 - 1/t) * alpha_{t} + eta * y_j * e_j\n",
    "        else:\n",
    "            alpha_{t+1} = (1 - 1/t) * alpha_{t}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda5256b649568d47b0ada5f4982042f9e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}