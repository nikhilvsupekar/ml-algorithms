{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Nikhil Supekar\n",
    "NetID: ns4486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE_PATH = '../data/ridge_regression/ridge_regression_dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E[\\| x \\|^2] & = E[\\sum_{i=1}^{n} x_i^2] \\\\\n",
    "& = \\sum_{i=1}^{n} E[x_i^2] \\\\\n",
    "& = \\sum_{i=1}^{n} \\sum_{j=1, y_j\\in Range(x_i)}^{5} y_j^2 p(y_j) \\\\\n",
    "& = \\sum_{i=1}^{n} (-2)^2 \\frac{1}{5} + (-1)^2 \\frac{1}{5} + (-0)^2 \\frac{1}{5} + (1)^2 \\frac{1}{5} + (2)^2 \\frac{1}{5} \\\\\n",
    "& = \\sum_{i=1}^{n} 2 \\\\\n",
    "& = 2n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E[\\|x\\|_{\\infty}] & = E[max_{i} |x_i|] \\\\\n",
    "& = E[|x_{max}|] \\\\\n",
    "& = |-2| \\frac{1}{5} + |-1| \\frac{1}{5} + |0| \\frac{1}{5} + |1| \\frac{1}{5} + |2| \\frac{1}{5} \\\\\n",
    "& = \\frac{6}{5}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Cov(\\overrightarrow{x}) & = [Cov(x_i, x_j)]_{ij} \\\\\n",
    "& = [0] \\text{if}\\ i \\neq j \\text{(since $x_i$ are iid)}\\\\\n",
    "&   [Var(x_i) \\text{otherwise}] \\\\\n",
    "& = [0] \\text{if}\\ i \\neq j \\\\\n",
    "&   2 \\\\\n",
    "& = 2 I_n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E[(a - y)^2]& = E[(a - E[y] + E[y] - y)^2] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + E[2(a - E[y])(E[y] - y)] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[(a - E[y])(E[y] - y)] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[(a - E[y])(E[y] - y) \\mid y]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[a E[y] - a y - E[y]^2 + E[y] y] \\mid y]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[a E[y] \\mid y] - E[a y \\mid y] - E[E[y]^2 \\mid y] + E[E[y] y \\mid y]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[y \\mid y] E[a \\mid y] - y E[a \\mid y] - E[y]^2 + E[y]^2]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[y E[a \\mid y] - y E[a \\mid y] + 0] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[0] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(a - E[y])^2 \\geq 0 & \\Rightarrow E[(a - E[y])^2] \\geq 0 \\\\\n",
    "& \\Rightarrow E[(a - E[y])^2] + E[(E[y] - y)^2] \\geq E[(E[y] - y)^2] \\\\\n",
    "& \\Rightarrow E[(a - y)^2] \\geq E[(E[y] - y)^2] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[(a - y)^2] = E[(E[y] - y)^2] & \\Leftrightarrow E[(a - E[y])^2] = 0 \\\\\n",
    "& \\Leftrightarrow a = E[y] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "min_{a}(E[(a - y)^2]) = E[(E[y] - y)^2] \\\\\n",
    "argmin_{a}(E[(a - y)^2]) = E[y]\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "E[(a - y)^2 \\mid x] & = E[(a - E[y \\mid x] + E[y \\mid x] - y)^2 \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 + (E[y \\mid x] - y)^2 + 2(a - E[y \\mid x])(E[y \\mid x] - y) \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 E[(a - E[y \\mid x])(E[y \\mid x] - y) \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 E[a E[y \\mid x] - a y - E[y \\mid x]^2 + E[y \\mid x] y \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 (E[a E[y \\mid x] \\mid x] - E[a y \\mid x] - E[E[y \\mid x]^2 \\mid x] + E[E[y \\mid x] y \\mid x]) \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 (E[E[y \\mid x] E[a \\mid x] - a E[y \\mid x] - E[y \\mid x]^2 + E[y \\mid x]^2]) \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 (0) \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(a - E[y \\mid x])^2 \\geq 0 & \\Rightarrow E[(a - E[y \\mid x])^2 \\mid x] \\geq 0\\\\\n",
    "& \\Rightarrow E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] \\geq E[(E[y \\mid x] - y)^2 \\mid x]\\\\\n",
    "& \\Rightarrow E[(a - y)^2 \\mid x] \\geq E[(E[y \\mid x] - y)^2 \\mid x] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[(a - y)^2 \\mid x] = E[(E[y \\mid x] - y)^2 \\mid x] & \\Leftrightarrow E[(a - E[y \\mid x])^2 \\mid x] = 0 \\\\\n",
    "& \\Leftrightarrow a - E[y \\mid x] = 0 \\\\\n",
    "& \\Leftrightarrow a = E[y \\mid x] \\\\\n",
    "& \\Leftrightarrow f(x) = E[y \\mid x] \\\\\n",
    "& \\Rightarrow f^{*}(x) = E[y \\mid x] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature normalization\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized - test set after normalization\n",
    "    \"\"\"\n",
    "    num_instances = train.shape[0]\n",
    "    num_features = train.shape[1]\n",
    "    \n",
    "    mean_vec = np.apply_along_axis(np.mean, 0, train)\n",
    "    std_vec = np.apply_along_axis(np.std, 0, train)\n",
    "    \n",
    "    const_feature_indices = np.where(std_vec == 0)\n",
    "    train = np.delete(train, const_feature_indices, 1)\n",
    "    test = np.delete(test, const_feature_indices, 1)\n",
    "    mean_vec = np.delete(mean_vec, const_feature_indices, 0)\n",
    "    std_vec = np.delete(mean_vec, const_feature_indices, 0)\n",
    "    \n",
    "    train_normalized = (train - mean_vec) / std_vec\n",
    "    test_normalized = (test - mean_vec) / std_vec\n",
    "    \n",
    "    return train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\|X \\theta - y\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla J(\\theta) = \\frac{2}{m} X^{T}(X \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta + \\eta h) - J(\\theta) & = \\eta h \\nabla J(\\theta) \\\\\n",
    "& = \\frac{2 \\eta h}{m} X^{T} (X \\theta - y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{i+1} & = \\theta_{i} - \\eta \\nabla J(\\theta_{i}) \\\\\n",
    "& = \\theta_{i} - \\frac{2 \\eta}{m} X^{T} (X \\theta - y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the average square loss for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    return np.linalg.norm(np.dot(X, theta) - y) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss (as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    return np.transpose(X).dot(np.dot(X, theta) - y) * 2 / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Gradient checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm. Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1))\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicating whether the gradient is correct or not\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    \n",
    "    J = compute_square_loss\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        approx_grad[i] = (J(theta + epsilon * np.eye(1, num_features, i)) - J(theta - epsilon * np.eye(1, num_features, i))) / (2 * epsilon)\n",
    "    \n",
    "    return np.linalg.norm(approx_grad, true_gradient) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. \n",
    "    And check whether gradient_func(X, y, theta) returned the true \n",
    "    gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    true_gradient = gradient_func(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    \n",
    "    J = objective_func\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        approx_grad[i] = (J(theta + epsilon * np.eye(1, num_features, i)) - J(theta - epsilon * np.eye(1, num_features, i))) / (2 * epsilon)\n",
    "    \n",
    "    return np.linalg.norm(approx_grad, true_gradient) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Batch gradient descent\n",
    "def batch_grad_descent(X, y, alpha=0.1, num_step=1000, grad_check=False):\n",
    "    \"\"\"\n",
    "    In this question you will implement batch gradient descent to\n",
    "    minimize the average square loss objective.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        alpha - step size in gradient descent\n",
    "        num_step - number of steps to run\n",
    "        grad_check - a boolean value indicating whether checking the gradient when updating\n",
    "\n",
    "    Returns:\n",
    "        theta_hist - the history of parameter vector, 2D numpy array of size (num_step+1, num_features)\n",
    "                     for instance, theta in step 0 should be theta_hist[0], theta in step (num_step) is theta_hist[-1]\n",
    "        loss_hist - the history of average square loss on the data, 1D numpy array, (num_step+1)\n",
    "    \"\"\"\n",
    "    num_instances, num_features = X.shape[0], X.shape[1]\n",
    "    theta_hist = np.zeros((num_step+1, num_features)) #Initialize theta_hist\n",
    "    loss_hist = np.zeros(num_step+1) #Initialize loss_hist\n",
    "    theta = np.zeros(num_features) #Initialize theta\n",
    "    \n",
    "    for i in range(1, num_step + 1):\n",
    "        theta = theta_hist[i - 1, :] - alpha * compute_square_loss_gradient(X, y, theta_hist[i - 1, :])\n",
    "        theta_hist[i, :] = theta\n",
    "        loss_hist[i] = compute_square_loss(X, y, theta)\n",
    "    \n",
    "    return theta_hist, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into Train and Test\n",
      "Scaling all to [0, 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(INPUT_FILE_PATH, delimiter=',')\n",
    "X = df.values[:,:-1]\n",
    "y = df.values[:,-1]\n",
    "\n",
    "print('Split into Train and Test')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =100, random_state=10)\n",
    "\n",
    "print(\"Scaling all to [0, 1]\")\n",
    "X_train, X_test = feature_normalization(X_train, X_test)\n",
    "X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1))))  # Add bias term\n",
    "X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))  # Add bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1f90a6a0>]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "STEP_SIZES = [0.005, 0.007, 0.009, 0.01, 0.018]\n",
    "avg_loss = []\n",
    "for alpha in STEP_SIZES:\n",
    "    theta, loss = batch_grad_descent(X_train, y_train, alpha)\n",
    "    avg_loss.append(np.mean(loss))\n",
    "# mean_vec = np.apply_along_axis(np.mean, 0, train).reshape(num_instances, 1)\n",
    "# std_vec = np.apply_along_axis(np.std, 0, train).reshape(num_instances, 1)\n",
    "plt.plot(STEP_SIZES, avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\|X \\theta - y\\|^2 + \\lambda \\theta^T \\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\nabla J(\\theta) & = \\frac{2}{m} X^{T}(X \\theta - y) + 2 \\lambda \\theta \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\theta_{i+1} & = \\theta_{i} - \\eta \\nabla J(\\theta_{i}) \\\\\n",
    "& = \\theta_{i} - \\frac{2 \\eta}{m} X^{T} (X \\theta - y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
