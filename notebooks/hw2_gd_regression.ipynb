{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Nikhil Supekar\n",
    "NetID: ns4486"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E[\\| x \\|^2] & = E[\\sum_{i=1}^{n} x_i^2] \\\\\n",
    "& = \\sum_{i=1}^{n} E[x_i^2] \\\\\n",
    "& = \\sum_{i=1}^{n} \\sum_{j=1, y_j\\in Range(x_i)}^{5} y_j^2 p(y_j) \\\\\n",
    "& = \\sum_{i=1}^{n} (-2)^2 \\frac{1}{5} + (-1)^2 \\frac{1}{5} + (-0)^2 \\frac{1}{5} + (1)^2 \\frac{1}{5} + (2)^2 \\frac{1}{5} \\\\\n",
    "& = \\sum_{i=1}^{n} 2 \\\\\n",
    "& = 2n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E[\\|x\\|_{\\infty}] & = E[max_{i} |x_i|] \\\\\n",
    "& = E[|x_{max}|] \\\\\n",
    "& = |-2| \\frac{1}{5} + |-1| \\frac{1}{5} + |0| \\frac{1}{5} + |1| \\frac{1}{5} + |2| \\frac{1}{5} \\\\\n",
    "& = \\frac{6}{5}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Cov(\\overrightarrow{x}) & = [Cov(x_i, x_j)]_{ij} \\\\\n",
    "& = [0] \\text{if}\\ i \\neq j \\text{(since $x_i$ are iid)}\\\\\n",
    "&   [Var(x_i) \\text{otherwise}] \\\\\n",
    "& = [0] \\text{if}\\ i \\neq j \\\\\n",
    "&   2 \\\\\n",
    "& = 2 I_n\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "E[(a - y)^2]& = E[(a - E[y] + E[y] - y)^2] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + E[2(a - E[y])(E[y] - y)] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[(a - E[y])(E[y] - y)] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[(a - E[y])(E[y] - y) \\mid y]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[a E[y] - a y - E[y]^2 + E[y] y] \\mid y]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[a E[y] \\mid y] - E[a y \\mid y] - E[E[y]^2 \\mid y] + E[E[y] y \\mid y]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[E[y \\mid y] E[a \\mid y] - y E[a \\mid y] - E[y]^2 + E[y]^2]] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[y E[a \\mid y] - y E[a \\mid y] + 0] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] + 2 E[0] \\\\\n",
    "& = E[(a - E[y])^2] + E[(E[y] - y)^2] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(a - E[y])^2 \\geq 0 & \\Rightarrow E[(a - E[y])^2] \\geq 0 \\\\\n",
    "& \\Rightarrow E[(a - E[y])^2] + E[(E[y] - y)^2] \\geq E[(E[y] - y)^2] \\\\\n",
    "& \\Rightarrow E[(a - y)^2] \\geq E[(E[y] - y)^2] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E[(a - y)^2] = E[(E[y] - y)^2] & \\Leftrightarrow E[(a - E[y])^2] = 0 \\\\\n",
    "& \\Leftrightarrow a = E[y] \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "min_{a}(E[(a - y)^2]) = E[(E[y] - y)^2] \\\\\n",
    "argmin_{a}(E[(a - y)^2]) = E[y]\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "E[(a - y)^2 \\mid x] & = E[(a - E[y \\mid x] + E[y \\mid x] - y)^2 \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 + (E[y \\mid x] - y)^2 + 2(a - E[y \\mid x])(E[y \\mid x] - y) \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 E[(a - E[y \\mid x])(E[y \\mid x] - y) \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 E[a E[y \\mid x] - a y - E[y \\mid x]^2 + E[y \\mid x] y \\mid x] \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 (E[a E[y \\mid x] \\mid x] - E[a y \\mid x] - E[E[y \\mid x]^2 \\mid x] + E[E[y \\mid x] y \\mid x]) \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 (E[E[y \\mid x] E[a \\mid x] - a E[y \\mid x] - E[y \\mid x]^2 + E[y \\mid x]^2]) \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] + 2 (0) \\\\\n",
    "& = E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "(a - E[y \\mid x])^2 \\geq 0 & \\Rightarrow E[(a - E[y \\mid x])^2 \\mid x] \\geq 0\\\\\n",
    "& \\Rightarrow E[(a - E[y \\mid x])^2 \\mid x] + E[(E[y \\mid x] - y)^2 \\mid x] \\geq E[(E[y \\mid x] - y)^2 \\mid x]\\\\\n",
    "& \\Rightarrow E[(a - y)^2 \\mid x] \\geq E[(E[y \\mid x] - y)^2 \\mid x] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "E[(a - y)^2 \\mid x] = E[(E[y \\mid x] - y)^2 \\mid x] & \\Leftrightarrow E[(a - E[y \\mid x])^2 \\mid x] = 0 \\\\\n",
    "& \\Leftrightarrow a - E[y \\mid x] = 0 \\\\\n",
    "& \\Leftrightarrow a = E[y \\mid x] \\\\\n",
    "& \\Leftrightarrow f(x) = E[y \\mid x] \\\\\n",
    "& \\Rightarrow f^{*}(x) = E[y \\mid x] \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature normalization\n",
    "def feature_normalization(train, test):\n",
    "    \"\"\"Rescale the data so that each feature in the training set is in\n",
    "    the interval [0,1], and apply the same transformations to the test\n",
    "    set, using the statistics computed on the training set.\n",
    "\n",
    "    Args:\n",
    "        train - training set, a 2D numpy array of size (num_instances, num_features)\n",
    "        test - test set, a 2D numpy array of size (num_instances, num_features)\n",
    "\n",
    "    Returns:\n",
    "        train_normalized - training set after normalization\n",
    "        test_normalized - test set after normalization\n",
    "    \"\"\"\n",
    "    num_instances = train.shape[0]\n",
    "    num_features = train.shape[1]\n",
    "    \n",
    "    mean_vec = np.apply_along_axis(np.mean, 0, train).reshape(num_instances, 1)\n",
    "    std_vec = np.apply_along_axis(np.std, 0, train).reshape(num_instances, 1)\n",
    "    \n",
    "    const_feature_indices = np.where(std_vec == 0)\n",
    "    train = np.delete(train, const_feature_indices, 1)\n",
    "    test = np.detele(test, const_feature_indices, 1)\n",
    "    \n",
    "    train_normalized = (train - mean_vec) / std_vec\n",
    "    test_normalized = (test - mean_vec) / std_vec\n",
    "    \n",
    "    return train_normalized, test_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J(\\theta) = \\frac{1}{m} \\|X \\theta - y\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla J(\\theta) = \\frac{2}{m} X^{T}(X \\theta - y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta + \\eta h) - J(\\theta) & = \\eta h \\nabla J(\\theta) \\\\\n",
    "& = \\frac{2 \\eta h}{m} X^{T} (X \\theta - y) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss(X, y, theta):\n",
    "    \"\"\"\n",
    "    Given a set of X, y, theta, compute the average square loss for predicting y with X*theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        loss - the average square loss, scalar\n",
    "    \"\"\"    \n",
    "    m = X.shape[0]\n",
    "    return np.linalg.norm(np.dot(X, theta) - y) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_square_loss_gradient(X, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the average square loss (as defined in compute_square_loss), at the point theta.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "\n",
    "    Returns:\n",
    "        grad - gradient vector, 1D numpy array of size (num_features)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    return np.transpose(X).dot(np.dot(X, theta) - y) * 2 / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### Gradient checker\n",
    "#Getting the gradient calculation correct is often the trickiest part\n",
    "#of any gradient-based optimization algorithm. Fortunately, it's very\n",
    "#easy to check that the gradient calculation is correct using the\n",
    "#definition of gradient.\n",
    "#See http://ufldl.stanford.edu/wiki/index.php/Gradient_checking_and_advanced_optimization\n",
    "def grad_checker(X, y, theta, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"Implement Gradient Checker\n",
    "    Check that the function compute_square_loss_gradient returns the\n",
    "    correct gradient for the given X, y, and theta.\n",
    "\n",
    "    Let d be the number of features. Here we numerically estimate the\n",
    "    gradient by approximating the directional derivative in each of\n",
    "    the d coordinate directions:\n",
    "    (e_1 = (1,0,0,...,0), e_2 = (0,1,0,...,0), ..., e_d = (0,...,0,1))\n",
    "\n",
    "    The approximation for the directional derivative of J at the point\n",
    "    theta in the direction e_i is given by:\n",
    "    ( J(theta + epsilon * e_i) - J(theta - epsilon * e_i) ) / (2*epsilon).\n",
    "\n",
    "    We then look at the Euclidean distance between the gradient\n",
    "    computed using this approximation and the gradient computed by\n",
    "    compute_square_loss_gradient(X, y, theta).  If the Euclidean\n",
    "    distance exceeds tolerance, we say the gradient is incorrect.\n",
    "\n",
    "    Args:\n",
    "        X - the feature vector, 2D numpy array of size (num_instances, num_features)\n",
    "        y - the label vector, 1D numpy array of size (num_instances)\n",
    "        theta - the parameter vector, 1D numpy array of size (num_features)\n",
    "        epsilon - the epsilon used in approximation\n",
    "        tolerance - the tolerance error\n",
    "\n",
    "    Return:\n",
    "        A boolean value indicating whether the gradient is correct or not\n",
    "    \"\"\"\n",
    "    true_gradient = compute_square_loss_gradient(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    \n",
    "    J = compute_square_loss\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        approx_grad[i] = (J(theta + epsilon * np.eye(1, num_features, i)) - J(theta - epsilon * np.eye(1, num_features, i))) / (2 * epsilon)\n",
    "    \n",
    "    return np.linalg.norm(approx_grad, true_gradient) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_gradient_checker(X, y, theta, objective_func, gradient_func, epsilon=0.01, tolerance=1e-4):\n",
    "    \"\"\"\n",
    "    The functions takes objective_func and gradient_func as parameters. \n",
    "    And check whether gradient_func(X, y, theta) returned the true \n",
    "    gradient for objective_func(X, y, theta).\n",
    "    Eg: In LSR, the objective_func = compute_square_loss, and gradient_func = compute_square_loss_gradient\n",
    "    \"\"\"\n",
    "    true_gradient = gradient_func(X, y, theta) #The true gradient\n",
    "    num_features = theta.shape[0]\n",
    "    approx_grad = np.zeros(num_features) #Initialize the gradient we approximate\n",
    "    \n",
    "    J = objective_func\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        approx_grad[i] = (J(theta + epsilon * np.eye(1, num_features, i)) - J(theta - epsilon * np.eye(1, num_features, i))) / (2 * epsilon)\n",
    "    \n",
    "    return np.linalg.norm(approx_grad, true_gradient) <= tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
