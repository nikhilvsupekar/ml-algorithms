{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### QUESTION 1 ###############\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"Returns each line of file as a list of strings\"\"\"\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        print(\"File {} doesn't exist\".format(file_path))\n",
    "    \n",
    "    file_lines = []\n",
    "    \n",
    "    with open(file_path) as fp:\n",
    "        for line in fp:\n",
    "            file_lines.append(line)\n",
    "    \n",
    "    return file_lines\n",
    "\n",
    "\n",
    "def train_val_split_sequential(file_path, ratio):\n",
    "    \"\"\"\n",
    "    Splits a file into training and validation data by a specified ratio\n",
    "    Train-validation splits are returned as lists of string\n",
    "    \"\"\"\n",
    "    \n",
    "    data = read_text_file(file_path)\n",
    "    train_size = int(len(data) * ratio)\n",
    "    train_data = data[0:train_size]\n",
    "    val_data = data[train_size:]\n",
    "    \n",
    "    return (train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_val_split_sequential('../data/email_spam/spam_train.txt', 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a validation set allows us to tune the hyperparameters of the model to increase the accuracy.\n",
    "This is something that cannot be done on the test set because ideally the test should should purely be used to report the final performance of the model. Using the test set while training can induce a bias in the model and the model may not generalize well beyond the given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### QUESTION 2 ###############\n",
    "\n",
    "def build_vocab(data, threshold=0):\n",
    "    \"\"\"\n",
    "    Takes input as a list of strings and returns a vocab of distinct words occurring in it.\n",
    "    Ignores all words occurring less than the threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = {}\n",
    "    \n",
    "    for line in data:\n",
    "        for word in line.split(' '):\n",
    "            vocab[word] = (1 if word not in vocab else (vocab[word] + 1))\n",
    "    \n",
    "    # remove the class names from vocab\n",
    "    for class_ in CLASSES:\n",
    "        vocab.pop(class_, None)\n",
    "    \n",
    "    # remove all keys with counts below a certain threshold\n",
    "    if threshold > 0:\n",
    "        for word in list(vocab.keys()):\n",
    "            if vocab[word] < threshold:\n",
    "                vocab.pop(word, None)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def transform_doc_to_features(doc, vocab_keys):\n",
    "    \"\"\"\n",
    "    Transforms a single text document into a feature vector.\n",
    "    The vectorizer is a binary vectorizer i.e. the features takes value 1 if the word is\n",
    "        present in the text and 0 if the word is absent.\n",
    "    \"\"\"\n",
    "    \n",
    "    ret_list = []\n",
    "    doc_class = doc.split(' ')[0]\n",
    "    doc_words = doc.split(' ')[1:]\n",
    "    \n",
    "    ret_list.append(doc_class)\n",
    "    \n",
    "    for key in vocab_keys:\n",
    "        if key in doc_words:\n",
    "            ret_list.append(1)\n",
    "        else:\n",
    "            ret_list.append(0)\n",
    "    \n",
    "    return ret_list\n",
    "\n",
    "def transform_text_to_features(data, master_vocab_keys=[]):\n",
    "    \"\"\"\n",
    "    Transforms text data into feature vectors by building a dictionary if a dictionary is not provided.\n",
    "    The vectorizer is a binary vectorizer i.e. the features takes value 1 if the word is\n",
    "        present in the text and 0 if the word is absent.\n",
    "    Input is a list of strings.\n",
    "    Output is a dataframe of word features.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(master_vocab_keys) == 0:\n",
    "        vocab = build_vocab(data, 30)\n",
    "        vocab_keys = list(vocab.keys())\n",
    "    else:\n",
    "        vocab_keys = list(master_vocab.keys())\n",
    "    \n",
    "    vocab_keys.sort()\n",
    "    features = ['__target__']\n",
    "    features.extend(vocab_keys)\n",
    "    transformed_list = []\n",
    "    \n",
    "    for doc in data:\n",
    "        transformed_list.append(transform_doc_to_features(doc, vocab_keys))\n",
    "    \n",
    "    return pd.DataFrame(transformed_list, columns = features), vocab_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, vocab_keys = transform_text_to_features(train_data)\n",
    "val_df, _ = transform_text_to_features(val_data, vocab_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### QUESTION 3 ###############\n",
    "\n",
    "def perceptron_train(data):\n",
    "    \"\"\"\n",
    "    Trains a perceptron model on the input dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    num_features = len(data.columns)\n",
    "    w = np.zeros(num_features - 1)\n",
    "    num_iterations = 0\n",
    "    num_updates = 0\n",
    "    \n",
    "    train_features = list(data.columns)\n",
    "    train_features.remove('__target__')\n",
    "    \n",
    "    error_flag = True\n",
    "    while error_flag:\n",
    "        error_flag = False\n",
    "        for index, row in data.iterrows():\n",
    "            y = int(row['__target__'])\n",
    "            x = row[train_features].to_numpy()\n",
    "            \n",
    "            if y == 0 and np.dot(w, x) >= 0:\n",
    "                w = w - x\n",
    "                error_flag = True\n",
    "                num_updates += 1\n",
    "            elif y == 1 and np.dot(w, x) < 0:\n",
    "                w = w + x\n",
    "                error_flag = True\n",
    "                num_updates += 1\n",
    "        \n",
    "        num_iterations += 1\n",
    "    \n",
    "    return (w, num_updates, num_iterations)\n",
    "\n",
    "\n",
    "def perceptron_test(w, data):\n",
    "    \"\"\"\n",
    "    Tests a trained perceptron model with weights w on a dataset\n",
    "    Returns the test error [number of misclassified samples]\n",
    "    \"\"\"\n",
    "    \n",
    "    train_features = list(data.columns)\n",
    "    train_features.remove('__target__')\n",
    "    \n",
    "    test_error = 0\n",
    "    for index, row in data.iterrows():\n",
    "        y = int(row['__target__'])\n",
    "        x = row[train_features].to_numpy()\n",
    "        \n",
    "        if (y == 0 and np.dot(w, x) >= 0) or (y == 1 and np.dot(w, x) < 0):\n",
    "            test_error += 1\n",
    "    \n",
    "    test_error /= len(data.index)\n",
    "    \n",
    "    return test_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning weights from the Perceptron Algorithm...\n",
      "number of mistakes by the Perceptron Algorithm while learning = 392\n",
      "\n",
      "checking the error on the training set...\n",
      "number of mistakes the training set after learning = 0.0\n",
      "\n",
      "testing the Perceptron Algorithm on the validation set...\n",
      "error on the validation set = 0.015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############### QUESTION 4 ###############\n",
    "\n",
    "print(\"learning weights from the Perceptron Algorithm...\")\n",
    "weights, num_mistakes, num_iterations = perceptron_train(train_df)\n",
    "print(\"number of mistakes by the Perceptron Algorithm while learning = {}\\n\".format(num_mistakes))\n",
    "\n",
    "print(\"checking the error on the training set...\")\n",
    "training_error = perceptron_test(weights, train_df)\n",
    "print(\"number of mistakes the training set after learning = {}\\n\".format(training_error))\n",
    "\n",
    "print(\"testing the Perceptron Algorithm on the validation set...\")\n",
    "validation_error = perceptron_test(weights, val_df)\n",
    "print(\"error on the validation set = {}\\n\".format(validation_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features with the smallest weights\n",
      "-19.0\tsuppos\n",
      "-14.0\tnumberdari\n",
      "-14.0\tstrateg\n",
      "-12.0\thabit\n",
      "-12.0\tkept\n",
      "-12.0\tque\n",
      "-12.0\tsnumber\n",
      "-11.0\tamount\n",
      "-11.0\tcomplain\n",
      "-11.0\tgood\n",
      "-11.0\treplica\n",
      "-10.0\tbritish\n",
      "-10.0\tcool\n",
      "-10.0\tminist\n",
      "-10.0\tpackag\n",
      "-10.0\tpocket\n",
      "\n",
      "features with the largest weights\n",
      "16.0\tcf\n",
      "16.0\tnot\n",
      "16.0\tpatch\n",
      "16.0\tpure\n",
      "14.0\textra\n",
      "13.0\tcorrupt\n",
      "13.0\tjame\n",
      "13.0\tmiddl\n",
      "13.0\tmom\n",
      "12.0\thelvetica\n",
      "12.0\tlearn\n",
      "11.0\tactiv\n",
      "11.0\tbearer\n",
      "11.0\tfear\n",
      "11.0\tfor\n",
      "11.0\tladi\n"
     ]
    }
   ],
   "source": [
    "############### QUESTION 5 ###############\n",
    "\n",
    "def get_top_dict_values(dict_, k, order = 0):\n",
    "    \"\"\"\n",
    "    Sorts a dict and returns the top k elems as a list of tuples (ordered by values)\n",
    "    order = 0 for ascending, order = 1 for descending\n",
    "    \"\"\"\n",
    "    \n",
    "    # map the order from {0, 1} to {-1, 1}\n",
    "    order = order * 2 - 1\n",
    "    \n",
    "    d = {k:v for k, v in sorted(dict_.items(), key = lambda item : -order*item[1])}\n",
    "    ret_list = []\n",
    "    for x in list(d)[0:(k + 1)]:\n",
    "        ret_list.append((x, d[x]))\n",
    "    \n",
    "    return ret_list\n",
    "\n",
    "def print_top_dict_values(dict_, k, order = 0):\n",
    "    \"\"\"\n",
    "    Prints the top k elems of the dictionary (ordered by values)\n",
    "    order = 0 for ascending, order = 1 for descending\n",
    "    \"\"\"\n",
    "    \n",
    "    for x in get_top_dict_values(dict_, k, order):\n",
    "        print(\"{}\\t{}\".format(x[1], x[0]))\n",
    "\n",
    "feature_weight_map = dict(zip(vocab_keys, weights))\n",
    "\n",
    "print(\"features with the smallest weights\")\n",
    "print_top_dict_values(feature_weight_map, 15, 0)\n",
    "\n",
    "print(\"\\nfeatures with the largest weights\")\n",
    "print_top_dict_values(feature_weight_map, 15, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
