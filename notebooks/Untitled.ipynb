{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### QUESTION 1 ###############\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    \"\"\"Returns each line of file as a list of strings\"\"\"\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        print(\"File {} doesn't exist\".format(file_path))\n",
    "    \n",
    "    file_lines = []\n",
    "    \n",
    "    with open(file_path) as fp:\n",
    "        for line in fp:\n",
    "            file_lines.append(line)\n",
    "    \n",
    "    return file_lines\n",
    "\n",
    "\n",
    "def train_val_split_sequential(file_path, ratio):\n",
    "    \"\"\"\n",
    "    Splits a file into training and validation data by a specified ratio\n",
    "    Train-validation splits are returned as lists of string\n",
    "    \"\"\"\n",
    "    \n",
    "    data = read_text_file(file_path)\n",
    "    train_size = int(len(data) * ratio)\n",
    "    train_data = data[0:train_size]\n",
    "    val_data = data[train_size:]\n",
    "    \n",
    "    return (train_data, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_val_split_sequential('../data/email_spam/spam_train.txt', 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a validation set allows us to tune the hyperparameters of the model to increase the accuracy.\n",
    "This is something that cannot be done on the test set because ideally the test should should purely be used to report the final performance of the model. Using the test set while training can induce a bias in the model and the model may not generalize well beyond the given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### QUESTION 2 ###############\n",
    "\n",
    "def build_vocab(data, threshold=0):\n",
    "    \"\"\"\n",
    "    Takes input as a list of strings and returns a vocab of distinct words occurring in it.\n",
    "    Ignores all words occurring less than the threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = {}\n",
    "    \n",
    "    for line in data:\n",
    "        for word in line.split(' '):\n",
    "            vocab[word] = (1 if word not in vocab else (vocab[word] + 1))\n",
    "    \n",
    "    # remove all keys with counts below a certain threshold\n",
    "    if threshold > 0:\n",
    "        for word in list(vocab.keys()):\n",
    "            if vocab[word] < threshold:\n",
    "                vocab.pop(word, None)\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def transform_doc_to_features(doc, vocab_keys):\n",
    "    \"\"\"\n",
    "    Transforms a single text document into a feature vector.\n",
    "    The vectorizer is a binary vectorizer i.e. the features takes value 1 if the word is\n",
    "        present in the text and 0 if the word is absent.\n",
    "    \"\"\"\n",
    "    \n",
    "    ret_list = []\n",
    "    doc_words = doc.split(' ')\n",
    "    \n",
    "    for key in vocab_keys:\n",
    "        if key in doc_words:\n",
    "            ret_list.append(1)\n",
    "        else:\n",
    "            ret_list.append(0)\n",
    "            \n",
    "    return ret_list\n",
    "\n",
    "def transform_text_to_features(data):\n",
    "    \"\"\"\n",
    "    Transforms text data into feature vectors by building a dictionary.\n",
    "    The vectorizer is a binary vectorizer i.e. the features takes value 1 if the word is\n",
    "        present in the text and 0 if the word is absent.\n",
    "    Input is a list of strings.\n",
    "    Output is a dataframe of word features.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = build_vocab(data, 30)\n",
    "    vocab_keys = list(vocab.keys())\n",
    "    vocab_keys.sort()\n",
    "    transformed_list = []\n",
    "    \n",
    "    for doc in data:\n",
    "        transformed_list.append(transform_doc_to_features(doc, vocab_keys))\n",
    "    \n",
    "    return pd.DataFrame(transformed_list, columns = vocab_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_text_to_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
